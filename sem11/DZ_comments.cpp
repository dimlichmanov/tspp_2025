/*Несколько комментариев по домашнему заданию и в общем по теме RMA: 

Я дал такое домашнее задание не с целью получения большего ускорения, чем при использовании Send/Recv, а с целью демонстрации того, 
что через RMA при заданных условиях задание решается значительно проще. Оцените сами - когда процесс сгенерировал какой-либо ключ, 
он знает получателя, в то время как получатель в этот момент не может знать, сколько данных и каких к нему должно придти. 
Поэтому, в Send/Recv схеме необходимо сделать много действий, чтобы +- удобно разрешить это задание

В RMA, однако, при заданных условиях, грамотное решение задания также не очевидно. Интуитивно понятным способом решения является 
создание отдельной эпохи на каждый из генерируемых ключей. Однако, это далеко не самый оптимальный способ решения.

Поэтому, я призываю вас обратить внимание на пример 11_cas. Это, как я и говорил на семинаре, должно вам напомнить третью домашку, когда вы писали 
атомарные операции для обновления начала(или конца) списка. В этой задаче этот шаблон вам идеально подойдёт - вы хотите положить в конец контейнера значение, 
но не уверены, что кладёте в конец (поскольку другой процесс тоже стремится положить своё значение в конец)


Всё это подводит к идее о пассивной синхронизации в RMA, которую мы обсудили в конце занятия. 
Поскольку времени на тему потрачено не так много, объясню тут:
Пассивная синхронизация отличается от активной тем, что в пассивной target-процесс (тот, у которого окно) не участвует в операциях RMA вообще, 
в отличие от активной, где он в рамках коллективной операции ставит забор Win_fence. 

В пассивной синхронизации нам помогают функции 
MPI_Win_lock(MPI_LOCK_SHARED, 0, 0, win); и MPI_Win_lock_all(0, win)
Они начинают эпоху для окна на target-процессе (то есть явно говорят MPI, что теперь можно с origin-процесса читать и писать в окно)
MPI_Win_lock открывает для origin-процесса какое-то одно окно (номер процесса указан вторым аргументом), MPI_Win_lock_all - открывает все окна 
LOCK_SHARED говорит о том, что lock на какое-то окно может сделать не только один origin-процесс. Если уверены что один - тогда MPI_LOCK_EXCLUSIVE
Поcле этого, мы на origin-процессе должны сами определять, когда дать нашим Put Get и другим функциям завершиться.
Посмотрите пример 08
MPI_Win_flush(0, win); - мы тут говорим, что хотим видеть все наши операции завершёнными и ждём

При MPI_Win_unlock и MPI_Win_unlock_all (завершение пассивной эпохи) происходит неявный flush

Самая главная мысль, которая не должна вас покидать - все RMA операции асинхронны. 
Они когда-то что-то сделают с окном, но чтобы убедиться в том, что операция выполнена - нужен забор в случае активной синхронизации и flush()/unlock() в случае пассивной
Accumulate хоть и атомарен, единственное что вы гарантируете - что операции от разных origin друг другу не помешают. 

Теперь к функциям из 11 примера (они, кстати, тоже атомарны)
MPI_Fetch_and_op - делает атомарно две вещи сразу - делает математическое действие в окне и возвращает значение, которое было ДО этого действия. 
Подобная операция у нас уже была на 3 семинаре, но при работе с общей памятью в контексте атомарных операций

MPI_Compare_and_swap - то же самое, что и на 3 семинаре - операция пытается положить значение в окно. Если получается, то actual == expected. 
Если нет, то кладёт нам в actual новое значение, которое нам скажет, 
что состояние окна к моменту нашей записи (пока мы делали + 1) изменилось. Надо попробовать ещё раз положить значение (и так в цикле)

Какой бы вы подход к решению задания не выбрали, помните, что эпоху нужно начать и завершить. К вопросу после семинара - в каких-то реализациях 
MPI может и без начала эпохи сделать успешно RMA-операции, но в целом требование по стандарту - 
сделать fence (активная) или lock (пассивная) до RMA-операций. Это всё-таки больше, чем правило хорошего тона.

*/